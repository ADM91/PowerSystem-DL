# PowerSystem-RL #
This project aims to explore reinforcement learning (RL) methods on power system restoration at the transmission level.

A restoration simulator is built at the core of the project and serves as the environment with which the RL agents
interact.  The simulator is built in as general a way as possible so that any amount of system degradation can take place
before restoration intervention takes over.  This includes the possibility to recover from single line faults, system
islanding, and blackouts.  The simulator is built on the Matpower6.0 toolbox, using optimal power flow (OPF) between actions
to solve the steady state system with unique constraints.  A standing phase angle constraint before line reconnection
is implemented.

The envisioned progression of the project is to start with simple power system simulation
and RL models and gradually increase their complexities until computing limits or time constraints are
reached. The simplest IEEE power system models will be used as proof of concept because they are easy to conceptualize and visualize.
Namely the 14 bus test case is used in the proof of concept phase of the project.
The intention is then to incorporate a dynamic temporal simulation model to verify the validity of the action sequences
generated by the RL model. Visualization of the data and power system network will be emphasized because it facilitates
understanding of the problem and results.

## Dependencies ##
* Matpower 6.0
* matplotlib
* numpy
* oct2py

## Objective function ##
* Minimize monetary cost of restoration

## Control parameters ##
* Action sequence:
    * Line reconnection
    * Load reconnection
    * Generator reconnection ()

## Initial assumptions (these will be reduced in future implementations) ##
* A degraded system can be generated by removing lines randomly
* Steady state solution is sufficient and accurate (with intention to move to a dynamic model)
* Automated system protections are ignored (breaker line tripping, demand side management, etc.)
* Restorations actions occur in series
* Reconnection of islands does not require significant control actions (they are easily synchronized)
* All generators have black start capability (temporary assumption)

## Methods of RL that will be investigated ##
Need to find and read and cite papers on each method...
* Cross-entropy method (CEM)
* Policy Gradient (PG)
* Deep Q-Networks (DQN)
* Actor Critic Agents (AC, A3C)
